# hyper-avatar-ui

ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆã«ã¯çŸ­ã„ README ã®ã¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

# hyper-avatar-ui
ä¸»ãªæ©Ÿèƒ½ã¯api/ã€Seed-VCéŸ³å£°å¤‰æ›ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ³ãƒ‰ãƒ«ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚Šã¾ã™ã€‚READMEã«ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®æ¨™ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚

Currently released model supports *zero-shot voice conversion* ...
The Streamlit chat UI now features an image upload button next to the text input. On mobile browsers you can take a photo or select one from your gallery and send it along with your message to GPT-4o.  
## InstallationğŸ“¥
Suggested python 3.10 on Windows, Mac M Series (Apple Silicon) or Linux.
Windows and Linux:
```bash
pip install -r requirements.txt

seedvc_service.py/convertã‚¨ãƒ³ãƒ‰/transcribeãƒã‚¤ãƒ³ãƒˆã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã® API ã‚­ãƒ¼èªè¨¼ã‚’å‚™ãˆãŸ FastAPI ã‚µãƒ¼ãƒãƒ¼ã‚’å…¬é–‹ã—ã¾ã™ã€‚

#  Seed-VC + Faster-Whisper FastAPI server  (API-KEY / resample fix)
API_KEY_ENV = "SEEDVC_API_KEY"               # ç’°å¢ƒå¤‰æ•°å
...
@app.post("/convert")
async def convert(
        src: UploadFile,
        model: str = Form(...),
        x_api_key: str | None = Header(None, alias="X-API-KEY")):
    """éŸ³å£°ã‚’ Seed-VC ã§å¤‰æ›ã— WAV ã‚’è¿”ã™"""

éŸ³å£°å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã¯ ã«å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™seed_vc_wrapper.pyã€‚ã“ã®convert_voiceãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®èª­ã¿è¾¼ã¿ã€ç‰¹å¾´æŠ½å‡ºã€æ¨è«–ã‚’å‡¦ç†ã—ã¾ã™ã€‚

def convert_voice(self, source, target, diffusion_steps=10, length_adjust=1.0,
                  inference_cfg_rate=0.7, f0_condition=False, auto_f0_adjust=True,
                  pitch_shift=0, stream_output=True):
    """Convert both timbre and voice from source to target."""
    ...
    source_audio = librosa.load(source, sr=sr)[0]
    ref_audio = librosa.load(target, sr=sr)[0]
    ...

ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯ã€ãƒã‚¤ã‚¯å…¥åŠ› â†’ Whisper â†’ ChatGPT â†’ Edge-TTS â†’ Seed-VC ã‚’é€£é–ã•ã›ã‚‹ui/Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ( ) ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚voice_bot_app5.py

#  voice_bot_app5.py
#    Browser mic â†’ Whisper(API) â†’ ChatGPT â†’ Edge-TTS â†’ Seed-VC(API)
...
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "...")
...
 VSTORE_PATH = {"aoki_model_v1": "hyponet_db", "sakaguchi_model_v1": "hyponet_db2"}

 @st.cache_resource
 def load_vectorstore(path: str):
     with st.spinner(f"ğŸ“š {path} ãƒ­ãƒ¼ãƒ‰ä¸­â€¦"):
         return FAISS.load_local(
             path,
             OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),
             allow_dangerous_deserialization=True,
         )

 def get_vectorstore(model_name: str) -> FAISS:
     path = VSTORE_PATH.get(model_name, "hyponet_db")
     return load_vectorstore(path)

åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å¾ŒåŠã§ã¯ã€ã‚¢ãƒ—ãƒªã¯ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚’ Seed-VC API ã«æŠ•ç¨¿ã—ã€å¤‰æ›ã•ã‚ŒãŸéŸ³å£°ã‚’å†ç”Ÿã—ã¾ã™ã€‚

# â‘¤ Seed-VC
r = requests.post(
    f"{API_HOST}/convert",
    headers=auth_headers,
    files={"src": ("voice.wav", open(RAW_WAV, "rb"), "audio/wav")},
    data={"model": model_name},
    timeout=120,
)
...
avatar_slot.image(GIF_TALK[model_name], use_container_width=True)
audio_slot.audio(io.BytesIO(wav_bytes), format="audio/wav", autoplay=True)

ç’°å¢ƒãƒ•ã‚¡ã‚¤ãƒ«ã¯Pythonã®ä¾å­˜é–¢ä¿‚ã‚’è¨˜è¿°ã—ã¾ã™ã€‚Streamlit UIã®å ´åˆï¼š

name: voice-ui
channels:
  - conda-forge        # conda-forge å„ªå…ˆ
dependencies:
  - python=3.10
  - streamlit=1.45
  - openai>=1.30
  - langchain-community>=0.3
  - langchain-openai>=0.3
  - edge-tts>=6.1,<7
  - faiss-cpu
  - pypdf
  - ffmpeg
  - pip:
    - edge-tts~=6.1

ã•ã‚‰ã«ã€ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆã«ã¯ã€ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãª pip è¦ä»¶ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™ã€‚

streamlit==1.45.1
edge-tts>=6.1,<7
openai>=1.30
requests>=2.31
langchain-community
langchain-openai
faiss-cpu
pypdf>=3.9

è£œåŠ©ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã¯mk-faiss.py ã¨ mk-faiss2.py ãŒã‚ã‚Šã€å‰è€…ã¯ docs/ ã® PDF ã‚’
hyponet_db ã«ã€å¾Œè€…ã¯ docs2/ ã® PDF ã‚’ hyponet_db2 ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã—ã¦ RAG ç”¨
ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

docs = []
for pdf_path in glob.glob("docs/*.pdf"):
    loader = PyPDFLoader(pdf_path)
    docs.extend(loader.load())
...
vect = FAISS.from_documents(chunks, embed)
vect.save_local("hyponet_db")
print(f"âœ… {len(chunks)} chunks indexed â†’ hyponet_db/")

docs = []
for pdf_path in glob.glob("docs2/*.pdf"):
    loader = PyPDFLoader(pdf_path)
    docs.extend(loader.load())
...
vect = FAISS.from_documents(chunks, embed)
vect.save_local("hyponet_db2")
print(f"âœ… {len(chunks)} chunks indexed â†’ hyponet_db2/")

å…¨ä½“ã¨ã—ã¦ã€ãƒªãƒã‚¸ãƒˆãƒªã¯æ¬¡ã®ã‚‚ã®ã‚’æä¾›ã—ã¾ã™ã€‚

api/seedvc_service.pySeed-VC ã¨ Faster-Whisper ã‚’å®Ÿè¡Œã™ã‚‹FastAPI ã‚µãƒ¼ãƒãƒ¼ ( )ã€‚

ui/voice_bot_app5.pyAPI ãŠã‚ˆã³ ChatGPT ã¨å¯¾è©±ã™ã‚‹Streamlit ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ ( )ã€‚

ã§ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡ã®ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨æ§‹æˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™api/ã€‚

Python ä¾å­˜é–¢ä¿‚ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®ç’°å¢ƒä»•æ§˜ã€‚

ã“ã‚Œã¯ã€æ–°ã—ã„è²¢çŒ®è€…ã«ã¨ã£ã¦ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’æ¢ç´¢ã™ã‚‹ãŸã‚ã®å‡ºç™ºç‚¹ã¨ãªã‚‹ã¯ãšã§ã™ã€‚ã«api/README.mdã¯è©³ç´°ãªä½¿ç”¨æ–¹æ³•ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹é †ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€UIãƒ•ã‚©ãƒ«ãƒ€ã«ã¯éŸ³å£°å¤‰æ›æ©Ÿèƒ½ã‚’Webã‚¢ãƒ—ãƒªã«çµ±åˆã™ã‚‹æ–¹æ³•ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
